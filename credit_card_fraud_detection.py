# -*- coding: utf-8 -*-
"""credit card fraud detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Opw6yUgfLGfme8HuxE-pZSV2cpDEE-wI
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import plotly.express as px
from sklearn.model_selection import cross_val_score

from sklearn import metrics
from collections import Counter

fraud_train = pd.read_csv('/content/fraudTrain.csv')
fraud_test = pd.read_csv('/content/fraudTest.csv')

fraud_train.head()

fraud_test.head()

print(fraud_test.shape)
print(fraud_train.shape)

fraud_test.isnull().sum()

fraud_train.isnull().sum()

fraud_test.info()

fraud_test.info()
fraud_train.info()

plt.figure(figsize=(15,15))
sns.heatmap(fraud_train.corr(), annot=True)

plt.scatter(fraud_train["zip"], fraud_train["long"])
plt.xlabel("ZIP")
plt.ylabel("LONG")
plt.show()

plt.scatter(fraud_train["is_fraud"], fraud_train["city_pop"])

fraud_train.columns

fig = px.pie(values = fraud_train['is_fraud'].value_counts(), names=["Genuine", "Fraud"], width=700, height=400, color_discrete_sequence=["skyblue", "black"], title="Fraud vs Genuine transactions")
fig.show()

plt.figure(figsize=(3,4))
ax = sns.countplot(x='is_fraud', data=fraud_train, palette="pastel")
for i in ax.containers:
  ax.bar_label(i,)

print('Genuine:', round(fraud_train['is_fraud'].value_counts()[0]/len(fraud_train)*100,2), '% of the dataset')
print('Frauds:', round(fraud_train['is_fraud'].value_counts()[1]/len(fraud_train) * 100,2), '% of the dataset')

fraud_train.info()
fraud_test.info()

fraud_train.isnull().sum()
fraud_test.isnull().sum()

drop_columns = ['Unnamed: 0', 'cc_num', 'merchant', 'trans_num', 'unix_time', 'first', 'last', 'street', 'zip']
fraud_train.drop(columns=drop_columns, inplace=True)
fraud_test.drop(columns=drop_columns, inplace=True)

fraud_train.shape

fraud_test.shape

fraud_train

fraud_train.job.value_counts()
fraud_train.job=pd.Categorical(fraud_train.job).codes
fraud_train

fraud_train['trans_date_trans_time'] = pd.to_datetime(fraud_train['trans_date_trans_time'])
fraud_train['trans_date'] = fraud_train['trans_date_trans_time'].dt.strftime('%Y-%m-%d')
fraud_train['trans_date'] = pd.to_datetime(fraud_train['trans_date'])
fraud_train['dob'] = pd.to_datetime(fraud_train['dob'])

fraud_test['trans_date_trans_time'] = pd.to_datetime(fraud_test['trans_date_trans_time'])
fraud_test['trans_date'] = fraud_test['trans_date_trans_time'].dt.strftime('%Y-%m-%d')
fraud_test['trans_date']=pd.to_datetime(fraud_test['trans_date'])
fraud_test['dob']=pd.to_datetime(fraud_test['dob'])

fraud_train["age"] = fraud_train["trans_date"]-fraud_train["dob"]
fraud_train["age"] = fraud_train["age"].astype('timedelta64[Y]')

fraud_test["age"] = fraud_test["trans_date"]-fraud_test["dob"]
fraud_test["age"] = fraud_test["age"].astype('timedelta64[Y]')

fraud_train['trans_month'] = pd.DatetimeIndex(fraud_train['trans_date']).month
fraud_train['trans_year'] = pd.DatetimeIndex(fraud_train['trans_date']).year

fraud_train['latitudinal_distance'] = abs(round(fraud_train['merch_lat']-fraud_train['lat'],3))
fraud_train['longitudinal_distance'] = abs(round(fraud_train['merch_long']-fraud_train['long'],3))

fraud_test[';atitudinal_distance'] = abs(round(fraud_test['merch_lat']-fraud_test['lat'],3))
fraud_test['longitudinal_distance'] = abs(round(fraud_test['merch_long']-fraud_test['long'],3))

drop_columns = ['trans_date_trans_time', 'city', 'lat', 'long', 'job', 'dob', 'merch_lat', 'trans_date', 'state']
fraud_train.drop(columns=drop_columns,inplace=True)
fraud_test.drop(columns=drop_columns,inplace=True)

fraud_train.gender=fraud_train.gender.apply(lambda x: 1 if x=="M" else 0)
fraud_test.gender=fraud_test.gender.apply(lambda x: 1 if x=="M" else 0)

fraud_train = pd.get_dummies(fraud_train, columns=['category'], prefix='category')
fraud_test = pd.get_dummies(fraud_test, columns=['category'], prefix='category')

fraud_test = fraud_test.reindex(columns=fraud_train.columns, fill_value=0)

fraud_train.head()

fraud_test.head()

X_train = fraud_train.drop('is_fraud', axis=1)
y_train = fraud_train['is_fraud']
X_test = fraud_test.drop('is_fraud', axis=1)
y_test = fraud_test['is_fraud']

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
x_test = scaler.transform(X_test)

def clean_dataset(fraud_train):
    assert isinstance(fraud_train, pd.DataFrame), "df needs to be a pd.DataFrame"
    fraud_train.dropna(inplace=True)
    indices_to_keep = fraud_train.isin([np.nan, np.inf, -np.inf]).any(axis=1)
    return fraud_train[indices_to_keep].astype(np.float64)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)

import numpy as np

# Replace NaN values with a specific value, e.g., 0
X_train = np.nan_to_num(X_train, nan=0)

X_train = X_train.dropna()
y_train = y_train.dropna()

from sklearn.ensemble import HistGradientBoostingClassifier

clf = HistGradientBoostingClassifier(random_state=42)
clf.fit(X_train, y_train)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(y_train)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

repory = classification_report(y_test, y_pred)

print(report)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train)

y_pred = clf.predict(X_test)
report = classification_report(X_test)

print(report)

"""# New Section"""

